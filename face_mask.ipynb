{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"face_mask.ipynb","provenance":[],"authorship_tag":"ABX9TyPpFUHBHcD2CKlZscd6i/TL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YonatN9aMIS1","executionInfo":{"status":"ok","timestamp":1625179066555,"user_tz":420,"elapsed":26388,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}},"outputId":"32fbdd03-1562-44ec-a755-f9bb097142fe"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2C5bICg5Nw7R"},"source":["train_dr ='/content/gdrive/MyDrive/Colab Notebooks/final_module/Train'\n","test_dr= '/content/gdrive/MyDrive/Colab Notebooks/final_module/Test'\n","validation_dr= '/content/gdrive/MyDrive/Colab Notebooks/final_module/Validation'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNUkpxzlOA6O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625179071501,"user_tz":420,"elapsed":11,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}},"outputId":"59ee2c2d-6cb6-4593-951f-3c249b8cfb60"},"source":["import numpy as np\n","import pandas as pd\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","np.arange(0,10,2)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 2, 4, 6, 8])"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"eU_4XVnFON4l","executionInfo":{"status":"ok","timestamp":1625179074253,"user_tz":420,"elapsed":1406,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}}},"source":["import tensorflow as tf\n","from tensorflow import keras"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"vP_2uH3sOVQS"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRtPQibAObTU","executionInfo":{"status":"ok","timestamp":1624790262203,"user_tz":-330,"elapsed":22359,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}},"outputId":"ec2f586c-7553-4805-a35a-db21c4577f8c"},"source":["train_datagen= ImageDataGenerator( rescale=1./255,\n","                                  rotation_range=40,\n","                                  height_shift_range=0.2,\n","                                  width_shift_range=0.2,\n","                                  shear_range=0.2,\n","                                  zoom_range=0.2,\n","                                  horizontal_flip=True)\n","test_datagen= ImageDataGenerator(rescale=1./255)\n","\n","train_generator= train_datagen.flow_from_directory(\n","    train_dr,\n","    target_size=(128,128),\n","    batch_size=20,\n","    class_mode='binary'\n",")\n","validation_generator= test_datagen.flow_from_directory(\n","    validation_dr,\n","    target_size=(128,128),\n","    batch_size=20,\n","    class_mode='binary'\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 10000 images belonging to 2 classes.\n","Found 800 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FbXtyX8tRTE8"},"source":["from tensorflow.keras.applications import VGG19"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R97EWlPcRh2U","executionInfo":{"elapsed":6810,"status":"ok","timestamp":1624777374891,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"},"user_tz":-330},"outputId":"b7d2e6dd-54e1-4afc-dabb-6b7c43a1cc96"},"source":["conv_base= VGG19(\n","    weights='imagenet',\n","    include_top=False,\n","    input_shape=(128,128,3)\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JUTfWTEORyIw"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras import models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEeb-LlUR7tu"},"source":["model= models.Sequential()\n","model.add(conv_base)\n","model.add(layers.Flatten())\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(2, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5LzVvQSSU31","executionInfo":{"elapsed":11,"status":"ok","timestamp":1624779582263,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"},"user_tz":-330},"outputId":"81e32d8e-f782-49be-8e27-cc81e9ebba8a"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg19 (Functional)           (None, 4, 4, 512)         20024384  \n","_________________________________________________________________\n","flatten (Flatten)            (None, 8192)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               2097408   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 22,122,049\n","Trainable params: 22,122,049\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LhVQGYhXSXio"},"source":["model.compile(loss='binary_crossentropy',\n","              optimizer= keras.optimizers.RMSprop(learning_rate=2e-5),\n","              metrics=['acc'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BATwB1wRSuvv"},"source":["checkpoint_cb= keras.callbacks.ModelCheckpoint('/content/gdrive/MyDrive/Colab Notebooks/final_module/face_mask.h5', save_best_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ti1e66zXS5rQ","outputId":"22378901-0c6b-47c3-957e-0fb71bd90154"},"source":["model_history= model.fit(train_generator,\n","                         steps_per_epoch=100,\n","                         epochs=10,\n","                         validation_data= validation_generator,\n","                         \n","                         callbacks=[checkpoint_cb])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","100/100 [==============================] - 382s 4s/step - loss: 0.0914 - acc: 0.9700 - val_loss: 0.0060 - val_acc: 0.9975\n","Epoch 2/10\n","100/100 [==============================] - 308s 3s/step - loss: 0.0390 - acc: 0.9880 - val_loss: 0.0030 - val_acc: 1.0000\n","Epoch 3/10\n","100/100 [==============================] - 232s 2s/step - loss: 0.0398 - acc: 0.9845 - val_loss: 0.0010 - val_acc: 1.0000\n","Epoch 4/10\n","100/100 [==============================] - 198s 2s/step - loss: 0.0261 - acc: 0.9925 - val_loss: 0.0025 - val_acc: 0.9987\n","Epoch 5/10\n","100/100 [==============================] - 176s 2s/step - loss: 0.0276 - acc: 0.9940 - val_loss: 0.0103 - val_acc: 0.9975\n","Epoch 6/10\n","100/100 [==============================] - 143s 1s/step - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0100 - val_acc: 0.9950\n","Epoch 7/10\n","100/100 [==============================] - 110s 1s/step - loss: 0.0200 - acc: 0.9915 - val_loss: 2.4394e-04 - val_acc: 1.0000\n","Epoch 8/10\n","100/100 [==============================] - 93s 928ms/step - loss: 0.0218 - acc: 0.9925 - val_loss: 0.0228 - val_acc: 0.9975\n","Epoch 9/10\n","100/100 [==============================] - 70s 700ms/step - loss: 0.0223 - acc: 0.9965 - val_loss: 3.9621e-04 - val_acc: 1.0000\n","Epoch 10/10\n","100/100 [==============================] - 65s 648ms/step - loss: 0.0289 - acc: 0.9900 - val_loss: 0.0118 - val_acc: 0.9975\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gv3JH54JaD-H"},"source":["del model\n","keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kwz-hnBHA6Nc","executionInfo":{"status":"ok","timestamp":1625179091170,"user_tz":420,"elapsed":8914,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}}},"source":["k = keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/final_module/face_mask.h5')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WVZbKN7jCqkm","executionInfo":{"status":"ok","timestamp":1625179101380,"user_tz":420,"elapsed":559,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}},"outputId":"f5ce895d-5f62-4f84-bb73-fabe249a32c6"},"source":["k.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg19 (Functional)           (None, 4, 4, 512)         20024384  \n","_________________________________________________________________\n","flatten (Flatten)            (None, 8192)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               2097408   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 22,122,049\n","Trainable params: 22,122,049\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmPqEBnzCtqF","executionInfo":{"status":"ok","timestamp":1624790671542,"user_tz":-330,"elapsed":303296,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}},"outputId":"a4bf90ab-b74b-4e94-9332-6215398aefba"},"source":["test_generator= test_datagen.flow_from_directory(\n","    test_dr,\n","    target_size=(128,128),\n","    batch_size=20,\n","    class_mode='binary'\n",")\n","\n","k.evaluate(test_generator, steps=50)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 992 images belonging to 2 classes.\n","50/50 [==============================] - 297s 5s/step - loss: 0.0135 - acc: 0.9970\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.013533997349441051, 0.9969757795333862]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"Sg4fDpwtziOA","executionInfo":{"status":"error","timestamp":1624936962730,"user_tz":-330,"elapsed":3083,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}},"outputId":"5fe8e261-db82-4e32-c131-15ddc4efcb76"},"source":["from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","from imutils.video import VideoStream\n","import numpy as np\n","import imutils\n","import time\n","import cv2\n","import os\n","\n","def detect_and_predict_mask(frame, faceNet, maskNet):\n","\t# grab the dimensions of the frame and then construct a blob\n","\t# from it\n","\t(h, w) = frame.shape[:2]\n","\tblob = cv2.dnn.blobFromImage(frame, 1.0, (128, 128),\n","\t\t(104.0, 177.0, 123.0))\n","\n","\t# pass the blob through the network and obtain the face detections\n","\tfaceNet.setInput(blob)\n","\tdetections = faceNet.forward()\n","\tprint(detections.shape)\n","\n","\t# initialize our list of faces, their corresponding locations,\n","\t# and the list of predictions from our face mask network\n","\tfaces = []\n","\tlocs = []\n","\tpreds = []\n","\n","\t# loop over the detections\n","\tfor i in range(0, detections.shape[2]):\n","\t\t# extract the confidence (i.e., probability) associated with\n","\t\t# the detection\n","\t\tconfidence = detections[0, 0, i, 2]\n","\n","\t\t# filter out weak detections by ensuring the confidence is\n","\t\t# greater than the minimum confidence\n","\t\tif confidence > 0.5:\n","\t\t\t# compute the (x, y)-coordinates of the bounding box for\n","\t\t\t# the object\n","\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\n","\t\t\t# ensure the bounding boxes fall within the dimensions of\n","\t\t\t# the frame\n","\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n","\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n","\t\t\t# ordering, resize it to 224x224, and preprocess it\n","\t\t\tface = frame[startY:endY, startX:endX]\n","\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","\t\t\tface = cv2.resize(face, (128, 128))\n","\t\t\tface = img_to_array(face)\n","\t\t\tface = preprocess_input(face)\n","\n","\t\t\t# add the face and bounding boxes to their respective\n","\t\t\t# lists\n","\t\t\tfaces.append(face)\n","\t\t\tlocs.append((startX, startY, endX, endY))\n","\n","\t# only make a predictions if at least one face was detected\n","\tif len(faces) > 0:\n","\t\t# for faster inference we'll make batch predictions on *all*\n","\t\t# faces at the same time rather than one-by-one predictions\n","\t\t# in the above `for` loop\n","\t\tfaces = np.array(faces, dtype=\"float32\")\n","\t\tpreds = maskNet.predict(faces, batch_size=32)\n","\n","\t# return a 2-tuple of the face locations and their corresponding\n","\t# locations\n","\treturn (locs, preds)\n","\n","# load our serialized face detector model from disk\n","prototxtPath = '/content/gdrive/MyDrive/Colab Notebooks/final_module/face_detector/deploy.prototxt'\n","weightsPath = '/content/gdrive/MyDrive/Colab Notebooks/final_module/face_detector/res10_300x300_ssd_iter_140000.caffemodel'\n","faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n","\n","# load the face mask detector model from disk\n","maskNet = k\n","\n","# initialize the video stream\n","print(\"[INFO] starting video stream...\")\n","vs = VideoStream(src=0).start()\n","\n","# loop over the frames from the video stream\n","while True:\n","\t# grab the frame from the threaded video stream and resize it\n","\t# to have a maximum width of 400 pixels\n","\tframe = vs.read()\n","\tframe = imutils.resize(frame, width=400)\n","\n","\t# detect faces in the frame and determine if they are wearing a\n","\t# face mask or not\n","\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n","\n","\t# loop over the detected face locations and their corresponding\n","\t# locations\n","\tfor (box, pred) in zip(locs, preds):\n","\t\t# unpack the bounding box and predictions\n","\t\t(startX, startY, endX, endY) = box\n","\t\t(mask, withoutMask) = pred\n","\n","\t\t# determine the class label and color we'll use to draw\n","\t\t# the bounding box and text\n","\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n","\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\n","\t\t# include the probability in the label\n","\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","\n","\t\t# display the label and bounding box rectangle on the output\n","\t\t# frame\n","\t\tcv2.putText(frame, label, (startX, startY - 10),\n","\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","\n","\t# show the output frame\n","\tcv2.imshow(\"Frame\", frame)\n","\tkey = cv2.waitKey(1) & 0xFF\n","\n","\t# if the `q` key was pressed, break from the loop\n","\tif key == ord(\"q\"):\n","\t\tbreak\n","\n","# do a bit of cleanup\n","cv2.destroyAllWindows()\n","vs.stop()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO] starting video stream...\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a81c16a81613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# to have a maximum width of 400 pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# detect faces in the frame and determine if they are wearing a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imutils/convenience.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# grab the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# if both the width and height are None, then return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"]}]},{"cell_type":"code","metadata":{"id":"e4aN-rzODaox","executionInfo":{"status":"ok","timestamp":1625179115264,"user_tz":420,"elapsed":2691,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}}},"source":["# import the necessary packages\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","from imutils.video import VideoStream\n","import numpy as np\n","import argparse\n","import imutils\n","import time\n","import cv2\n","import os\n","\n","def detect_and_predict_mask(frame, faceNet, maskNet):\n","\t# grab the dimensions of the frame and then construct a blob\n","\t# from it\n","\t(h, w) = frame.shape[:2]\n","\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n","\t\t(104.0, 177.0, 123.0))\n","\n","\t# pass the blob through the network and obtain the face detections\n","\tfaceNet.setInput(blob)\n","\tdetections = faceNet.forward()\n","\n","\t# initialize our list of faces, their corresponding locations,\n","\t# and the list of predictions from our face mask network\n","\tfaces = []\n","\tlocs = []\n","\tpreds = []\n","\n","\t# loop over the detections\n","\tfor i in range(0, detections.shape[2]):\n","\t\t# extract the confidence (i.e., probability) associated with\n","\t\t# the detection\n","\t\tconfidence = detections[0, 0, i, 2]\n","\n","\t\t# filter out weak detections by ensuring the confidence is\n","\t\t# greater than the minimum confidence\n","\t\tif confidence > 0.5:\n","\t\t\t# compute the (x, y)-coordinates of the bounding box for\n","\t\t\t# the object\n","\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\n","\t\t\t# ensure the bounding boxes fall within the dimensions of\n","\t\t\t# the frame\n","\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n","\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n","\t\t\t# ordering, resize it to 224x224, and preprocess it\n","\t\t\tface = frame[startY:endY, startX:endX]\n","\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","\t\t\tface = cv2.resize(face, (128, 128))\n","\t\t\tface = img_to_array(face)\n","\t\t\tface = preprocess_input(face)\n","\n","\t\t\t# add the face and bounding boxes to their respective\n","\t\t\t# lists\n","\t\t\tfaces.append(face)\n","\t\t\tlocs.append((startX, startY, endX, endY))\n","\n","\t# only make a predictions if at least one face was detected\n","\tif len(faces) > 0:\n","\t\t# for faster inference we'll make batch predictions on *all*\n","\t\t# faces at the same time rather than one-by-one predictions\n","\t\t# in the above `for` loop\n","\t\tfaces = np.array(faces, dtype=\"float32\")\n","\t\tpreds = maskNet.predict(faces, batch_size=32)\n","\n","\t# return a 2-tuple of the face locations and their corresponding\n","\t# locations\n","\treturn (locs, preds)\n","\n","faceNet=cv2.dnn.readNet('/content/gdrive/MyDrive/Colab Notebooks/final_module/face_detector/deploy.prototxt','/content/gdrive/MyDrive/Colab Notebooks/final_module/face_detector/res10_300x300_ssd_iter_140000.caffemodel')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEEwgUWEmMqO","executionInfo":{"status":"ok","timestamp":1625179121173,"user_tz":420,"elapsed":593,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}}},"source":["import base64\n","import html\n","import io\n","import time\n","\n","from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","import numpy as np\n","from PIL import Image\n","import cv2\n","\n","def start_input():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 512; //video.videoWidth;\n","      captureCanvas.height = 512; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function takePhoto(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def take_photo(label, img_data):\n","  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n","  return data"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZ9cMXthmYVc","executionInfo":{"status":"ok","timestamp":1625179129251,"user_tz":420,"elapsed":767,"user":{"displayName":"nandan singh","photoUrl":"","userId":"00362712206507476503"}}},"source":["\n","def js_reply_to_image(js_reply):\n","    \"\"\"\n","    input: \n","          js_reply: JavaScript object, contain image from webcam\n","    output: \n","          image_array: image array RGB size 512 x 512 from webcam\n","    \"\"\"\n","    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n","    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n","    image_array = np.array(image_PIL)\n","\n","    return image_array"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9t6a4Fbmbvu"},"source":["import imutils\n","start_input()\n","label_html = 'Capturing...'\n","img_data = ''\n","count = 0 \n","from google.colab.patches import cv2_imshow\n","while True:\n","  js_reply = take_photo(label_html, img_data)\n","  if not js_reply:\n","    break\n","    \n","  image = js_reply_to_image(js_reply)\n","\n","\t# grab the frame from the threaded video stream and resize it\n","\t# to have a maximum width of 400 pixels\n","  frame = image\n","  v=True\n","  if v == True:\n","\n","    frame = imutils.resize(frame, width=400)\n","\n","\t# detect faces in the frame and determine if they are wearing a\n","\t# face mask or not\n","    (locs, preds) = detect_and_predict_mask(frame, faceNet, k)\n","    for (box, pred) in zip(locs, preds):\n","\n","\n","\t\t# unpack the bounding box and predictions\n","      (startX, startY, endX, endY) = box\n","      (mask,) = pred\n","\n","\t\t# determine the class label and color we'll use to draw\n","\t\t# the bounding box and text\n","      label = \"Mask\" if mask <0.01 else \"No Mask\"\n","      color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\n","\t\t# include the probability in the label\n","      \n","\n","\t\t# display the label and bounding box rectangle on the output\n","\t\t# frame\n","      frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","      frame=cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","\n","\t# show the output frame\n","      cv2_imshow(frame)"],"execution_count":null,"outputs":[]}]}